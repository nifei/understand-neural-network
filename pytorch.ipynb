{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/nifei/understand-neural-network/blob/master/pytorch.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CF1-nXBupAU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2c008fc1-6258-448e-a458-5ae46553671a"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\r\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.2.0)\r\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vWBUD4Yd4zQx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " * <b>例子1: </b>首先是对 1x1的tensor x 到 1x1 的 tensor y 的反向求导. loss 取值为 神经元 y的输出的mean, 是一个标量. "
      ]
    },
    {
      "metadata": {
        "id": "ibxihn20orsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46528e0d-8bd6-4879-94a5-d9056cbff2a7"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.ones(1, 1, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J6bCz954plii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd477ce0-bd97-420a-d560-292355c4cf3b"
      },
      "cell_type": "code",
      "source": [
        "y = x * x + x\n",
        "y"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "hTHGO1Fmp6-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fdcaac7-b872-4127-b044-4a3248491f88"
      },
      "cell_type": "code",
      "source": [
        "out = y.mean()\n",
        "out"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "i9wt71YRp_JY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "d25ac296-82fa-4507-8144-c56fe4b58608"
      },
      "cell_type": "code",
      "source": [
        "out.backward()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rZf5HcghqBRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c75c553b-d852-4b13-df76-621c960abf73"
      },
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "_SuEcF-N5Nac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* <b>例子2:</b> 接着尝试对 多神经元层 到多神经元层 反向求导. x 为 2元tensor, y是二元tensor.  y[i] = f(x[i]), 此处缺图. "
      ]
    },
    {
      "metadata": {
        "id": "dYOZbGp-53mc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a6c388f7-1183-4267-a6d6-4614a0994913"
      },
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor([[1, 2]])\n",
        "x.requires_grad=True\n",
        "print(x)\n",
        "y = x * x\n",
        "print (y)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.,  2.]])\n",
            "tensor([[ 1.,  4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ONuXQ5Pk6V6w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "假设目标为\"使得y层的神经元都是0\", 那么 loss[i]=y[i] - 0, 对 loss 做bp, 运行时错误为:\n",
        "<code> grad can be implicitly created only for scalar outputs </code>, 因为loss为2x1的tensor. "
      ]
    },
    {
      "metadata": {
        "id": "8pGMtQtS6k6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = y\n",
        "loss.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZNLxyLH47bU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "但是这里,你照葫芦画瓢, 令loss = y.mean(), 是有问题的. 因为不管优化的目标是x层到y层的weights矩阵, 还是输入层x, 单步优化的幅度对x[0]和x[1] , 或者对不同连接的权重, 肯定是不一样的, x[0]应该减1, x[1]应该减2. y.mean() 显然会丢失这个信息. \n",
        "\n",
        "以优化目标为x层到y层的权重为例, 我们期望的, weights 的grad应该是: \n",
        "\n",
        "$$[[\\frac{dy[0]}{dx[0]}, \\frac{dy[0]}{dx[1]}], [\\frac{dy[1]}{dx[0]}, \\frac{dy[1]}{dx[1]}]]$$\n",
        "\n",
        "如果优化目标是x层的输入, 我想了一下但是不知道怎么表达, 反正在4个weights固定的情况下, 你是没有办法把输入优化到y层输出完全符合期望的. \n",
        "\n",
        "// 举个栗子, y[0] = x[0] + x[1], y[1] = x[0] + x[1] 这样的权重固定的前提下, 你是没有办法, 通过优化 x, 来达到 y = [0, 1] 的目标的. \n",
        "\n",
        "所以退而求其次, 只能追求, y和期望的偏差距离最小. 此处用 L1 距离, 也就是y.mean()来作为loss. \n"
      ]
    },
    {
      "metadata": {
        "id": "FqBtUALL7VNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d373fc6f-5d23-48fc-cf6b-4a13f42bd658"
      },
      "cell_type": "code",
      "source": [
        "loss = y.mean()\n",
        "print(loss)\n",
        "loss.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.,  2.]])\n",
            "tensor([[ 1.,  4.]])\n",
            "tensor(2.5000)\n",
            "tensor([[ 1.,  2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "npNufVQQBX2F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "这个结果还算符合预期, 因为:\n",
        "$$[[\\frac{dy[0]}{dx[0]}, \\frac{dy[1]}{dx[1]}]] = 2, 4$$\n",
        "被平均后各自变成原来的一半也可以理解. \n",
        "\n",
        "* <b>例子3:</b>为了验证这个想法, 我们取sum(y)作为loss, 执行下一个单元后看到输出的x.grad是符合预期的 [2, 4]. "
      ]
    },
    {
      "metadata": {
        "id": "CxfmC3X1Bs_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4a4064cf-ff37-4b97-bb0a-807b80f7e271"
      },
      "cell_type": "code",
      "source": [
        "x = torch.FloatTensor([[1, 2]])\n",
        "x.requires_grad=True\n",
        "print(x)\n",
        "y = x * x\n",
        "print (y)\n",
        "loss = y.sum()\n",
        "print(loss)\n",
        "loss.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.,  2.]])\n",
            "tensor([[ 1.,  4.]])\n",
            "tensor(5.)\n",
            "tensor([[ 2.,  4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "waM9XLBzCrcV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "不要高兴的太早, 以上的例子 y[i] = fi(x[i]) 的算子都是一样的, 如果网络不是这样的, 就没那么乐观了. \n",
        "* <b>例子4:</b> 此处缺图, x, y 都是2元 tensor, y[0] = x[0] + x[1], y[1] = x[0] - x[1], 所以\n",
        "$$[[\\frac{dy[0]}{dx[0]}, \\frac{dy[0]}{dx[1]}], [\\frac{dy[1]}{dx[0]}, \\frac{dy[1]}{dx[1]}]] = [[1, 1], [1, -1]]$$\n",
        "显然以下单元的代码执行的结果, x.grad在维度上和结果上都不足够表达梯度求导的信息. "
      ]
    },
    {
      "metadata": {
        "id": "k-_6xjVqDGOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ec6d0d5f-edef-4f7e-e783-327bc79d4333"
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "x = Variable(torch.IntTensor([[1, 2]]), requires_grad=True)\n",
        "print(x)\n",
        "y = Variable(torch.zeros(1, 2))\n",
        "y[0, 0] = x[0, 0] + x[0, 1]\n",
        "y[0, 1] = x[0, 0] - x[0, 1]\n",
        "print (y)\n",
        "loss = y.sum()\n",
        "print(loss)\n",
        "loss.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1,  2]], dtype=torch.int32)\n",
            "tensor([[ 3., -1.]])\n",
            "tensor(2.)\n",
            "tensor([[ 2,  0]], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vYYqImLTFYbR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* <b>例子5: </b> 求单个神经元对输入的梯度. 即以最大/最小化 y[i] 为目标, 优化输入层 x. \n",
        "直接考虑是把y[i] 作为loss. 以下代码输出符合预期. "
      ]
    },
    {
      "metadata": {
        "id": "2a9N2_k7KCby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "83301b46-f433-4c32-9b26-d63d7defee65"
      },
      "cell_type": "code",
      "source": [
        "x = Variable(torch.IntTensor([[1, 2]]), requires_grad=True)\n",
        "print(x)\n",
        "y = Variable(torch.zeros(1, 2))\n",
        "y[0, 0] = x[0, 0] + x[0, 1]\n",
        "y[0, 1] = x[0, 0] - x[0, 1]\n",
        "print (y)\n",
        "loss = y[0][1]\n",
        "print(loss)\n",
        "loss.backward()\n",
        "print(x.grad)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1,  2]], dtype=torch.int32)\n",
            "tensor([[ 3., -1.]])\n",
            "tensor(-1.)\n",
            "tensor([[ 1, -1]], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ylHCwG0AKOJc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "按照pytorch文档的说明, 向量y对向量x的求导, 需要指定向量y各分量的权重(grad_tensors), 然后它做一个加权求和.... 用这个和的标量对向量x求导... ... \n",
        "https://pytorch.org/docs/master/autograd.html#torch.autograd.Variable.backward\n",
        "\n",
        "也就是说, pytorch从根本上, 拒绝向量对向量的求导... 可能是觉得不值得吧. \n",
        "\n",
        "* <b>例子6:</b> 还是通过y层的单个神经元的loss来优化x, 只不过通过backward的grad_tensors指定. 这个参数还可以用来对y层全部加权求和, 求平均, 什么的... 以下单元代码输出符合预期, 为y[1]对x[0], x[1]的求导. "
      ]
    },
    {
      "metadata": {
        "id": "Rw4NhpDDL7ND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b634f4e4-f900-4682-b47f-9d0aaa89b2ba"
      },
      "cell_type": "code",
      "source": [
        "x = Variable(torch.IntTensor([[1, 2]]), requires_grad=True)\n",
        "print(x)\n",
        "y = Variable(torch.zeros(1, 2))\n",
        "y[0, 0] = x[0, 0] + x[0, 1]\n",
        "y[0, 1] = x[0, 0] - x[0, 1]\n",
        "print (y)\n",
        "loss = y\n",
        "print(loss)\n",
        "torch.autograd.backward(loss, grad_tensors=torch.FloatTensor([[0, 1]]))\n",
        "print(x.grad)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1,  2]], dtype=torch.int32)\n",
            "tensor([[ 3., -1.]])\n",
            "tensor([[ 3., -1.]])\n",
            "tensor([[ 1, -1]], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wbWVsGxthYmQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "无关: 乘法"
      ]
    },
    {
      "metadata": {
        "id": "yV9JAH3UhXgx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test0 = torch.zeros(2, 2)\n",
        "test0[0, 0] = 1\n",
        "test0[0, 1] = 1\n",
        "test1 = torch.zeros(2, 2)\n",
        "test1[0, 0] = 1\n",
        "test1[1, 0] = 1\n",
        "print(test0)\n",
        "print(test1)\n",
        "print(test0*test1)\n",
        "print(test0.mm(test1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yO0K51UTo8Dv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "3d0a3d65-eaa3-4434-ceaa-4d47c7724476"
      },
      "cell_type": "code",
      "source": [
        "def find_non_zero_rect(data, row_count, col_count):\n",
        "  min_r, min_c, max_r, max_c = None, None, None, None\n",
        "  for r in range(row_count):\n",
        "    for c in range(col_count):\n",
        "      if data[r, c] != 0:\n",
        "        if min_r is None:\n",
        "          min_r = r\n",
        "          max_r = r\n",
        "        else:\n",
        "          if min_r > r:\n",
        "            min_r = r\n",
        "          if max_r < r:\n",
        "            max_r = r\n",
        "        if min_c is None:\n",
        "          min_c = c\n",
        "          max_c = c\n",
        "        else:\n",
        "          if min_c > c:\n",
        "            min_c = c\n",
        "          if max_c < c:\n",
        "            max_c = c\n",
        "  if min_r is None or min_c is None or max_r is None or max_c is None:\n",
        "    return None, None, None, None\n",
        "  return min_r, min_c, max_r, max_c\n",
        "    "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2WB93gFBow1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "690d327e-53fc-4d5b-f140-f5a5e6dbea92"
      },
      "cell_type": "code",
      "source": [
        "test0 = torch.zeros(224, 224)\n",
        "test0[0, 0] = 1\n",
        "test0[1, 0] = 1\n",
        "test0[0, 1] = 1\n",
        "print(test0)\n",
        "print (find_non_zero_rect(test0, 224, 224))\n",
        "\n",
        "test0 = torch.zeros(50, 50)\n",
        "test0[10, 10] = 1\n",
        "test0[10, 20] = 1\n",
        "test0[9, 15] = 1\n",
        "print(test0)\n",
        "print (find_non_zero_rect(test0, 50, 50))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
            "        [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        ...,\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])\n",
            "(0, 0, 1, 1)\n",
            "tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        ...,\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])\n",
            "(9, 10, 10, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}